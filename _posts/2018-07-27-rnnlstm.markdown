---
layout: post
title: "RNN&LSTM"
date: "2018-07-27 16:56:14 +0900"
---
<script src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

- __RNN__: Recurrent Neural Network
- __LSTM__: Long Short-Term Memory models

# RNN

![RNN overview]({{ "http://i.imgur.com/s8nYcww.png" | absolute_url }})

$$t-1$$에 계산된 $$h_{t-1}$$ 값을 시간 $$t$$에 인풋으로 사용해서 $$h_t$$를 계산한다.
이를 통해서 시간 $$t$$에 그 당시의 인풋인 $$x_t$$ 뿐만 아니라 $$x_{t-1}$$ 등 이전의
인풋 값 또한 고려하여 $$y_{t}$$를 계산한다는 것을 알 수 있다. 또, 생각해보면 $$y_{t}$$
를 계산할 때, 시간 $$t$$에서 거리가 멀어질수록 그때의 인풋의 영향이 적어진다는 것을
알 수 있다.

activation function은 $$tanh$$ 함수를 사용한다.

# LSTM

RNN에서 문제점은 gradient descent algorithm을 사용해서 시간이 처음에 가까워질수록
($$t$$가 0에 가까워질수록) gradient(미분값)이 크게 줄기 때문에 weight의 업데이트가
매우 조금밖에 되지 않는다는 단점이 있다. 이것을 __vanishing gradient problem__ 이라고
부른다. 이것을 해결하기 위해서 고안된 것이 바로 __LSTM__ 이다.
